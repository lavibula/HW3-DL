{"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":30892,"databundleVersionId":2715462,"sourceType":"competition"}],"dockerImageVersionId":30580,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"papermill":{"default_parameters":{},"duration":2389.428167,"end_time":"2023-11-15T14:16:08.945990","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2023-11-15T13:36:19.517823","version":"2.4.0"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install segmentation-models-pytorch","metadata":{"papermill":{"duration":19.130253,"end_time":"2023-11-15T13:36:42.008247","exception":false,"start_time":"2023-11-15T13:36:22.877994","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-11-15T15:54:46.708164Z","iopub.execute_input":"2023-11-15T15:54:46.708440Z","iopub.status.idle":"2023-11-15T15:55:09.038008Z","shell.execute_reply.started":"2023-11-15T15:54:46.708413Z","shell.execute_reply":"2023-11-15T15:55:09.036374Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport cv2\nfrom torchvision.io import read_image\nimport matplotlib.pyplot as plt\nfrom torch.utils.data import Dataset, random_split, DataLoader\nimport albumentations as A\nfrom albumentations.pytorch.transforms import ToTensorV2\n\nfrom torchvision.transforms import ToTensor\nfrom PIL import Image\nimport os\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision \nfrom torchvision import transforms\nfrom torchinfo import summary\nimport timm","metadata":{"papermill":{"duration":7.445345,"end_time":"2023-11-15T13:36:49.462058","exception":false,"start_time":"2023-11-15T13:36:42.016713","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-11-15T15:55:09.040346Z","iopub.execute_input":"2023-11-15T15:55:09.040655Z","iopub.status.idle":"2023-11-15T15:55:19.325151Z","shell.execute_reply.started":"2023-11-15T15:55:09.040628Z","shell.execute_reply":"2023-11-15T15:55:19.324372Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!nvidia-smi -L","metadata":{"papermill":{"duration":0.99887,"end_time":"2023-11-15T13:36:50.469104","exception":false,"start_time":"2023-11-15T13:36:49.470234","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-11-15T15:55:19.326203Z","iopub.execute_input":"2023-11-15T15:55:19.326515Z","iopub.status.idle":"2023-11-15T15:55:20.331922Z","shell.execute_reply.started":"2023-11-15T15:55:19.326489Z","shell.execute_reply":"2023-11-15T15:55:20.330994Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport cv2\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nimport albumentations as A\nfrom albumentations.pytorch.transforms import ToTensorV2\nimport segmentation_models_pytorch as smp\nfrom torchinfo import summary\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nimport wandb\n\n# Constants\nNUM_CLASSES = 3\nIMAGE_SIZE = (256, 256)\nLEARNING_RATE = 0.0001\nBATCH_SIZE = 4\nNUM_EPOCHS = 50\n\n# Data paths\nTRAIN_PATH = '/kaggle/input/bkai-igh-neopolyp/train/train'\nTRAIN_MASK_PATH = '/kaggle/input/bkai-igh-neopolyp/train_gt/train_gt'\n\ncolor_dict = {\n    0: [0, 0, 0],  # Background\n    1: [255, 0, 0],  # Class 1 (Red)\n    2: [0, 255, 0],  # Class 2 (Green)\n    # Add more classes if necessary\n}\n# Utility Functions\n\ndef mask_to_rgb(mask, color_dict):\n    \"\"\"Converts mask to RGB image.\"\"\"\n    output = np.zeros((mask.shape[0], mask.shape[1], 3))\n\n    for k in color_dict.keys():\n        output[mask==k] = color_dict[k]\n\n    return np.uint8(output)\n\ndef save_best_model(epoch, model, optimizer, loss, save_path):\n    \"\"\"Saves the best model.\"\"\"\n    checkpoint = {\n        'epoch': epoch,\n        'model': model.state_dict(),\n        'optimizer': optimizer.state_dict(),\n        'loss': loss,\n    }\n    torch.save(checkpoint, save_path)\n\ndef get_device():\n    \"\"\"Returns the appropriate device (GPU or CPU).\"\"\"\n    return torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Dataset Classes\n\nclass UnetImageDataset(Dataset):\n    \"\"\"Custom dataset for images and labels.\"\"\"\n    def __init__(self, img_dir, label_dir, resize=None, transform=None):\n        \"\"\"\n        Args:\n            img_dir (str): Directory containing input images.\n            label_dir (str): Directory containing corresponding label masks.\n            resize (tuple): Desired image size (height, width).\n            transform (callable): Optional transform to be applied to the image.\n        \"\"\"\n        self.img_dir = img_dir\n        self.label_dir = label_dir\n        self.resize = resize\n        self.transform = transform\n        self.images = os.listdir(self.img_dir)\n\n    def __len__(self):\n        \"\"\"Returns the number of images in the dataset.\"\"\"\n        return len(self.images)\n\n    def read_mask(self, mask_path):\n        \"\"\"Reads and processes the mask image.\"\"\"\n        image = cv2.imread(mask_path)\n        image = cv2.resize(image, self.resize)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n\n        lower1 = np.array([0, 100, 20])\n        upper1 = np.array([10, 255, 255])\n\n        lower2 = np.array([160,100,20])\n        upper2 = np.array([179,255,255])\n        lower_mask = cv2.inRange(image, lower1, upper1)\n        upper_mask = cv2.inRange(image, lower2, upper2)\n        \n        red_mask = lower_mask + upper_mask;\n        red_mask[red_mask != 0] = 1\n\n        green_mask = cv2.inRange(image, (36, 25, 25), (70, 255, 255))\n        green_mask[green_mask != 0] = 2\n\n        full_mask = cv2.bitwise_or(red_mask, green_mask)\n        full_mask = np.expand_dims(full_mask, axis=-1) \n        full_mask = full_mask.astype(np.uint8)\n        \n        return full_mask\n\n    def __getitem__(self, idx):\n        \"\"\"Gets an image and its corresponding label at the given index.\"\"\"\n        img_path = os.path.join(self.img_dir, self.images[idx])\n        label_path = os.path.join(self.label_dir, self.images[idx])\n\n        # Read and preprocess the image\n        image = cv2.imread(img_path)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        image = cv2.resize(image, self.resize)\n\n        # Read and preprocess the label\n        label = self.read_mask(label_path)\n\n        if self.transform:\n            image = self.transform(image)\n\n        return image, label\n        \n    def show_image(self, idx):\n        \"\"\"Displays the original image and its label.\"\"\"\n        img_path = os.path.join(self.img_dir, self.images[idx])\n        label_path = os.path.join(self.label_dir, self.images[idx])\n\n        image = plt.imread(img_path)\n        label = plt.imread(label_path)\n\n        fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n        axs[0].imshow(image)\n        axs[0].set_title('Image')\n        axs[1].imshow(label)\n        axs[1].set_title('Label')\n        plt.show()\n\n# Extended Dataset Class\n\nclass UnetDataset(UnetImageDataset):\n    \"\"\"Extended dataset class.\"\"\"\n    def __init__(self, data, targets, transform=None):\n        \"\"\"\n        Args:\n            data (list): List of input images.\n            targets (list): List of corresponding label masks.\n            transform (callable): Optional transform to be applied to the image.\n        \"\"\"\n        self.data = data\n        self.targets = targets\n        self.transform = transform\n\n    def __getitem__(self, index):\n        \"\"\"Gets an image and its corresponding label at the given index.\"\"\"\n        image = self.data[index]\n        label = self.targets[index]\n\n        if self.transform:\n            transformed = self.transform(image=image, mask=label)\n            image = transformed['image'].float()\n            label = transformed['mask'].float()\n            label = label.permute(2, 0, 1)\n\n        return image, label\n\n    def __len__(self):\n        \"\"\"Returns the number of images in the dataset.\"\"\"\n        return len(self.data)\n\n# Data Augmentation\n\ntrain_transform = A.Compose([\n    A.HorizontalFlip(p=0.5),\n    A.VerticalFlip(p=0.5),\n    A.RandomGamma(gamma_limit=(70, 130), eps=None, always_apply=False, p=0.2),\n    A.RGBShift(p=0.3, r_shift_limit=10, g_shift_limit=10, b_shift_limit=10),\n    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n    ToTensorV2(),\n])\n\nval_transform = A.Compose([\n    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n    ToTensorV2(),\n])\n\n# Dataset Split and Creation\n\nimage_path = [os.path.join(root, file) for root, _, files in os.walk(TRAIN_PATH) for file in files]\nmask_path = [os.path.join(root, file) for root, _, files in os.walk(TRAIN_MASK_PATH) for file in files]\n\ndataset = UnetImageDataset(\n    img_dir=TRAIN_PATH,\n    label_dir=TRAIN_MASK_PATH,\n    resize=(256, 256),\n    transform=None\n)\n\nimages_data = []\nlabels_data = []\nfor x,y in dataset:\n    images_data.append(x)\n    labels_data.append(y)\n# Model Initialization\n\nmodel = smp.Unet(\n    encoder_name=\"resnet34\",\n    encoder_weights=\"imagenet\",\n    in_channels=3,\n    classes=NUM_CLASSES\n)\n\n# Data Loader Creation\n\ntrain_size = int(0.8 * len(dataset))\nval_size = len(dataset) - train_size\n\ntrain_dataset = UnetDataset(images_data[:train_size], labels_data[:train_size], transform=train_transform)\nval_dataset = UnetDataset(images_data[train_size:], labels_data[train_size:], transform=val_transform)\n\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True)\n\n# Model Training\n\ndevice = get_device()\nmodel.to(device)\noptimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\ncriterion = nn.CrossEntropyLoss()\nbest_val_loss = float('inf')\n\nwandb.login(\n    # set the wandb project where this run will be logged\n#     project= \"PolypSegment\", \n    key = \"b9575849263a9312a73f76d71d270c8751628e10\",\n)\nwandb.init(project='Unet_polyp-Segmentation')\n\n\nfor epoch in range(NUM_EPOCHS):\n    model.train()\n    train_loss_epoch = 0  # Initialize train_loss_epoch\n    for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS}\"):\n        images, labels = images.to(device), labels.to(device)\n        labels = labels.squeeze(dim=1).long()\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n        \n        train_loss_epoch += loss.item()  # Accumulate training loss for the epoch\n\n    model.eval()\n    with torch.no_grad():\n        val_loss = 0\n        for images, labels in val_loader:\n            images, labels = images.to(device), labels.to(device)\n            labels = labels.squeeze(dim=1).long()\n            outputs = model(images)\n            val_loss += criterion(outputs.float(), labels.long()).item()\n\n    avg_train_loss = train_loss_epoch / len(train_loader)  # Calculate average training loss\n    avg_val_loss = val_loss / len(val_loader)\n    print(f\"Epoch [{epoch+1}/{NUM_EPOCHS}], Train Loss: {avg_train_loss:.10f}, Valid Loss: {avg_val_loss:.10f}\")\n\n    if avg_val_loss < best_val_loss:\n        best_val_loss = avg_val_loss\n        save_best_model(epoch, model, optimizer, val_loss, 'colorization_model.pth')\n        print('Save new model')\n    wandb.log({\"Train loss\": avg_train_loss, \"Valid loss\": avg_val_loss})\n    \n# Model Inference on Test Data\n\ntrainsize = 256\nmodel.eval()\n\nfor i in os.listdir(\"/kaggle/input/bkai-igh-neopolyp/test/test\"):\n    img_path = os.path.join(\"/kaggle/input/bkai-igh-neopolyp/test/test\", i)\n    ori_img = cv2.imread(img_path)\n    ori_img = cv2.cvtColor(ori_img, cv2.COLOR_BGR2RGB)\n    ori_w = ori_img.shape[0]\n    ori_h = ori_img.shape[1]\n    img = cv2.resize(ori_img, (trainsize, trainsize))\n    transformed = val_transform(image=img)\n    input_img = transformed[\"image\"]\n    input_img = input_img.unsqueeze(0).to(device)\n    with torch.no_grad():\n        output_mask = model.forward(input_img).squeeze(0).cpu().numpy().transpose(1, 2, 0)\n    mask = cv2.resize(output_mask, (ori_h, ori_w))\n    mask = np.argmax(mask, axis=2)\n    new_rgb_mask = np.zeros((*mask.shape, 3)).astype(np.uint8)\n    mask_rgb = mask_to_rgb(mask, color_dict)\n    cv2.imwrite(\"predicted_mask/{}\".format(i), mask_rgb)\n    print(+1)\n","metadata":{"execution":{"iopub.status.busy":"2023-11-15T15:55:20.334382Z","iopub.execute_input":"2023-11-15T15:55:20.334708Z","iopub.status.idle":"2023-11-15T16:19:14.976197Z","shell.execute_reply.started":"2023-11-15T15:55:20.334678Z","shell.execute_reply":"2023-11-15T16:19:14.975027Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transform = A.Compose([\n    A.HorizontalFlip(p=0.5),\n    A.VerticalFlip(p=0.5),\n    A.RandomGamma(gamma_limit=(70, 130), eps=None, always_apply=False, p=0.2),\n    A.RGBShift(p=0.3, r_shift_limit=10, g_shift_limit=10, b_shift_limit=10),\n    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n    ToTensorV2(),\n])","metadata":{"execution":{"iopub.status.busy":"2023-11-15T16:26:15.336360Z","iopub.execute_input":"2023-11-15T16:26:15.337194Z","iopub.status.idle":"2023-11-15T16:26:15.346503Z","shell.execute_reply.started":"2023-11-15T16:26:15.337155Z","shell.execute_reply":"2023-11-15T16:26:15.345326Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nfrom torch.utils.data import Dataset\nfrom PIL import Image\nimport numpy as np\n\nclass UNetTestDataClass(Dataset):\n    def __init__(self, img_dir, transform=None, target_size=(256, 256)):\n        self.img_dir = img_dir\n        self.transform = transform\n        self.target_size = target_size\n        self.images = os.listdir(self.img_dir)\n\n    def __len__(self):\n        return len(self.images)\n\n    def __getitem__(self, index):\n        img_path = os.path.join(self.img_dir, self.images[index])\n        pil_image = Image.open(img_path).convert(\"RGB\")\n\n        # Resize image\n        pil_image = pil_image.resize(self.target_size, Image.BILINEAR)\n\n        h, w = self.target_size\n\n        # Convert PIL Image to numpy array\n        img_array = np.array(pil_image)\n\n        # Apply transformations\n        transformed_data = self.transform(image=img_array)\n        data = transformed_data[\"image\"] / 255  # Divide by 255 after applying the transformation\n\n        return data, img_path, h, w","metadata":{"execution":{"iopub.status.busy":"2023-11-15T16:32:35.288681Z","iopub.execute_input":"2023-11-15T16:32:35.289330Z","iopub.status.idle":"2023-11-15T16:32:35.299386Z","shell.execute_reply.started":"2023-11-15T16:32:35.289297Z","shell.execute_reply":"2023-11-15T16:32:35.298305Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"path = '/kaggle/input/bkai-igh-neopolyp/test/test/'\nunet_test_dataset = UNetTestDataClass(path, transform)\ntest_dataloader = DataLoader(unet_test_dataset, batch_size=8, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2023-11-15T16:32:37.612416Z","iopub.execute_input":"2023-11-15T16:32:37.613281Z","iopub.status.idle":"2023-11-15T16:32:37.621507Z","shell.execute_reply.started":"2023-11-15T16:32:37.613243Z","shell.execute_reply":"2023-11-15T16:32:37.620325Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i, (data, path, h, w) in enumerate(test_dataloader):\n    img = data\n    break","metadata":{"execution":{"iopub.status.busy":"2023-11-15T16:32:38.899770Z","iopub.execute_input":"2023-11-15T16:32:38.900693Z","iopub.status.idle":"2023-11-15T16:32:39.072237Z","shell.execute_reply.started":"2023-11-15T16:32:38.900650Z","shell.execute_reply":"2023-11-15T16:32:39.067684Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport torch\nimport torch.nn.functional as F\n\n# Assuming that 'img' and 'predict' are your tensors\nimg = img.cpu()  # Move img tensor to CPU\npredict = predict.cpu()  # Move predict tensor to CPU\n\n# Visualization code\nfig, arr = plt.subplots(5, 2, figsize=(10, 20))\n\nfor i in range(5):\n    arr[i][0].imshow(img[i].permute(1, 2, 0).numpy())\n    arr[i][1].imshow(F.one_hot(torch.argmax(predict[i], 0)).float().numpy())\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-11-15T16:33:42.139706Z","iopub.execute_input":"2023-11-15T16:33:42.140190Z","iopub.status.idle":"2023-11-15T16:33:43.928406Z","shell.execute_reply.started":"2023-11-15T16:33:42.140149Z","shell.execute_reply":"2023-11-15T16:33:43.927367Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport torch\nimport torchvision.transforms as transforms\nfrom torchvision.transforms import Resize, ToPILImage, InterpolationMode\nimport torch.nn.functional as F\n\n# Assuming 'b' is your input tensor and 'model' is your segmentation model\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\n# Move the model to the specified device\nmodel = model.to(device)\n\n# Move the input tensor to the same device as the model\nb = b.to(device)\n\n# Set the model to evaluation mode\nmodel.eval()\n\nif not os.path.isdir(\"/kaggle/working/predicted_masks\"):\n    os.mkdir(\"/kaggle/working/predicted_masks\")\nfor _, (img, path, H, W) in enumerate(test_dataloader):\n    a = path\n    b = img\n    h = H\n    w = W\n    \n    # Move input tensors to the device\n    b = b.to(device)\n    h = h.to(device)\n    w = w.to(device)\n\n    with torch.no_grad():\n        predicted_mask = model(b)\n    for i in range(len(a)):\n        image_id = a[i].split('/')[-1].split('.')[0]\n        filename = image_id + \".png\"\n        mask2img = Resize((h[i].item(), w[i].item()), interpolation=InterpolationMode.NEAREST)(ToPILImage()(F.one_hot(torch.argmax(predicted_mask[i], 0)).permute(2, 0, 1).float()))\n        mask2img.save(os.path.join(\"/kaggle/working/predicted_masks/\", filename))","metadata":{"execution":{"iopub.status.busy":"2023-11-15T16:37:23.624333Z","iopub.execute_input":"2023-11-15T16:37:23.624717Z","iopub.status.idle":"2023-11-15T16:37:29.287772Z","shell.execute_reply.started":"2023-11-15T16:37:23.624686Z","shell.execute_reply":"2023-11-15T16:37:29.286653Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def rle_to_string(runs):\n    return ' '.join(str(x) for x in runs)\n\ndef rle_encode_one_mask(mask):\n    pixels = mask.flatten()\n    pixels[pixels > 0] = 255\n    use_padding = False\n    if pixels[0] or pixels[-1]:\n        use_padding = True\n        pixel_padded = np.zeros([len(pixels) + 2], dtype=pixels.dtype)\n        pixel_padded[1:-1] = pixels\n        pixels = pixel_padded\n    \n    rle = np.where(pixels[1:] != pixels[:-1])[0] + 2\n    if use_padding:\n        rle = rle - 1\n    rle[1::2] = rle[1::2] - rle[:-1:2]\n    return rle_to_string(rle)\n\ndef mask2string(dir):\n    ## mask --> string\n    strings = []\n    ids = []\n    ws, hs = [[] for i in range(2)]\n    for image_id in os.listdir(dir):\n        id = image_id.split('.')[0]\n        path = os.path.join(dir, image_id)\n        print(path)\n        img = cv2.imread(path)[:,:,::-1]\n        h, w = img.shape[0], img.shape[1]\n        for channel in range(2):\n            ws.append(w)\n            hs.append(h)\n            ids.append(f'{id}_{channel}')\n            string = rle_encode_one_mask(img[:,:,channel])\n            strings.append(string)\n    r = {\n        'ids': ids,\n        'strings': strings,\n    }\n    return r\n\n\nMASK_DIR_PATH = '/kaggle/working/predicted_masks' # change this to the path to your output mask folder\ndir = MASK_DIR_PATH\nres = mask2string(dir)\ndf = pd.DataFrame(columns=['Id', 'Expected'])\ndf['Id'] = res['ids']\ndf['Expected'] = res['strings']\ndf.to_csv(r'output.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2023-11-15T16:37:31.822590Z","iopub.execute_input":"2023-11-15T16:37:31.822956Z","iopub.status.idle":"2023-11-15T16:37:32.072046Z","shell.execute_reply.started":"2023-11-15T16:37:31.822923Z","shell.execute_reply":"2023-11-15T16:37:32.070027Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}