{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e3ff341",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-15T17:37:15.296706Z",
     "iopub.status.busy": "2023-11-15T17:37:15.296375Z",
     "iopub.status.idle": "2023-11-15T17:37:34.592011Z",
     "shell.execute_reply": "2023-11-15T17:37:34.590710Z"
    },
    "papermill": {
     "duration": 19.3036,
     "end_time": "2023-11-15T17:37:34.594524",
     "exception": false,
     "start_time": "2023-11-15T17:37:15.290924",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting segmentation-models-pytorch\r\n",
      "  Obtaining dependency information for segmentation-models-pytorch from https://files.pythonhosted.org/packages/cb/70/4aac1b240b399b108ce58029ae54bc14497e1bbc275dfab8fd3c84c1e35d/segmentation_models_pytorch-0.3.3-py3-none-any.whl.metadata\r\n",
      "  Downloading segmentation_models_pytorch-0.3.3-py3-none-any.whl.metadata (30 kB)\r\n",
      "Requirement already satisfied: torchvision>=0.5.0 in /opt/conda/lib/python3.10/site-packages (from segmentation-models-pytorch) (0.15.1)\r\n",
      "Collecting pretrainedmodels==0.7.4 (from segmentation-models-pytorch)\r\n",
      "  Downloading pretrainedmodels-0.7.4.tar.gz (58 kB)\r\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25hCollecting efficientnet-pytorch==0.7.1 (from segmentation-models-pytorch)\r\n",
      "  Downloading efficientnet_pytorch-0.7.1.tar.gz (21 kB)\r\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25hCollecting timm==0.9.2 (from segmentation-models-pytorch)\r\n",
      "  Obtaining dependency information for timm==0.9.2 from https://files.pythonhosted.org/packages/29/90/94f5deb8d76e24a89813aef95e8809ca8fd7414490428480eda19b133d4a/timm-0.9.2-py3-none-any.whl.metadata\r\n",
      "  Downloading timm-0.9.2-py3-none-any.whl.metadata (68 kB)\r\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m68.5/68.5 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from segmentation-models-pytorch) (4.66.1)\r\n",
      "Requirement already satisfied: pillow in /opt/conda/lib/python3.10/site-packages (from segmentation-models-pytorch) (10.1.0)\r\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (2.0.0)\r\n",
      "Collecting munch (from pretrainedmodels==0.7.4->segmentation-models-pytorch)\r\n",
      "  Obtaining dependency information for munch from https://files.pythonhosted.org/packages/56/b3/7c69b37f03260a061883bec0e7b05be7117c1b1c85f5212c72c8c2bc3c8c/munch-4.0.0-py2.py3-none-any.whl.metadata\r\n",
      "  Downloading munch-4.0.0-py2.py3-none-any.whl.metadata (5.9 kB)\r\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from timm==0.9.2->segmentation-models-pytorch) (6.0.1)\r\n",
      "Requirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from timm==0.9.2->segmentation-models-pytorch) (0.17.3)\r\n",
      "Requirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from timm==0.9.2->segmentation-models-pytorch) (0.4.0)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision>=0.5.0->segmentation-models-pytorch) (1.24.3)\r\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torchvision>=0.5.0->segmentation-models-pytorch) (2.31.0)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (3.12.2)\r\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (4.5.0)\r\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (1.12)\r\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (3.1)\r\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (3.1.2)\r\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->timm==0.9.2->segmentation-models-pytorch) (2023.10.0)\r\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->timm==0.9.2->segmentation-models-pytorch) (21.3)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision>=0.5.0->segmentation-models-pytorch) (3.2.0)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision>=0.5.0->segmentation-models-pytorch) (3.4)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision>=0.5.0->segmentation-models-pytorch) (1.26.15)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision>=0.5.0->segmentation-models-pytorch) (2023.7.22)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface-hub->timm==0.9.2->segmentation-models-pytorch) (3.0.9)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (2.1.3)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (1.3.0)\r\n",
      "Downloading segmentation_models_pytorch-0.3.3-py3-none-any.whl (106 kB)\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m106.7/106.7 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading timm-0.9.2-py3-none-any.whl (2.2 MB)\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m38.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading munch-4.0.0-py2.py3-none-any.whl (9.9 kB)\r\n",
      "Building wheels for collected packages: efficientnet-pytorch, pretrainedmodels\r\n",
      "  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25h  Created wheel for efficientnet-pytorch: filename=efficientnet_pytorch-0.7.1-py3-none-any.whl size=16428 sha256=0133976f5463afa0cc45abcd0bc0ce0d09a881d80374e990ed812fc987ec21f4\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/03/3f/e9/911b1bc46869644912bda90a56bcf7b960f20b5187feea3baf\r\n",
      "  Building wheel for pretrainedmodels (setup.py) ... \u001b[?25l-\b \b\\\b \b|\b \bdone\r\n",
      "\u001b[?25h  Created wheel for pretrainedmodels: filename=pretrainedmodels-0.7.4-py3-none-any.whl size=60943 sha256=153875fdafbe61416a8f0babc2ea0a3740a2dea2f1d846e035553203f1e6f6cf\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/35/cb/a5/8f534c60142835bfc889f9a482e4a67e0b817032d9c6883b64\r\n",
      "Successfully built efficientnet-pytorch pretrainedmodels\r\n",
      "Installing collected packages: munch, efficientnet-pytorch, timm, pretrainedmodels, segmentation-models-pytorch\r\n",
      "  Attempting uninstall: timm\r\n",
      "    Found existing installation: timm 0.9.10\r\n",
      "    Uninstalling timm-0.9.10:\r\n",
      "      Successfully uninstalled timm-0.9.10\r\n",
      "Successfully installed efficientnet-pytorch-0.7.1 munch-4.0.0 pretrainedmodels-0.7.4 segmentation-models-pytorch-0.3.3 timm-0.9.2\r\n"
     ]
    }
   ],
   "source": [
    "!pip install segmentation-models-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6642a6f2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-15T17:37:34.609415Z",
     "iopub.status.busy": "2023-11-15T17:37:34.609101Z",
     "iopub.status.idle": "2023-11-15T17:37:41.937253Z",
     "shell.execute_reply": "2023-11-15T17:37:41.936369Z"
    },
    "papermill": {
     "duration": 7.338136,
     "end_time": "2023-11-15T17:37:41.939695",
     "exception": false,
     "start_time": "2023-11-15T17:37:34.601559",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "from torchvision.io import read_image\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, random_split, DataLoader\n",
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "\n",
    "from torchvision.transforms import ToTensor\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision \n",
    "from torchvision import transforms\n",
    "from torchinfo import summary\n",
    "import timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98dca896",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-15T17:37:41.954398Z",
     "iopub.status.busy": "2023-11-15T17:37:41.954128Z",
     "iopub.status.idle": "2023-11-15T17:37:42.935951Z",
     "shell.execute_reply": "2023-11-15T17:37:42.934959Z"
    },
    "papermill": {
     "duration": 0.991574,
     "end_time": "2023-11-15T17:37:42.938236",
     "exception": false,
     "start_time": "2023-11-15T17:37:41.946662",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0: Tesla T4 (UUID: GPU-55e9dc09-64ba-026c-6660-81b081817fa1)\r\n",
      "GPU 1: Tesla T4 (UUID: GPU-51dce878-37f7-2730-45ff-2700c5f16819)\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi -L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26ecaa55",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-15T17:37:42.954231Z",
     "iopub.status.busy": "2023-11-15T17:37:42.953953Z",
     "iopub.status.idle": "2023-11-15T17:48:59.439520Z",
     "shell.execute_reply": "2023-11-15T17:48:59.438428Z"
    },
    "papermill": {
     "duration": 676.496275,
     "end_time": "2023-11-15T17:48:59.441650",
     "exception": false,
     "start_time": "2023-11-15T17:37:42.945375",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet34-333f7ec4.pth\" to /root/.cache/torch/hub/checkpoints/resnet34-333f7ec4.pth\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 83.3M/83.3M [00:00<00:00, 216MB/s]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlavibuu\u001b[0m (\u001b[33mlavibu\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.16.0 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.12\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20231115_173829-exc5volm\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mtreasured-serenity-6\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/lavibu/Unet_polyp-Segmentation\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/lavibu/Unet_polyp-Segmentation/runs/exc5volm\u001b[0m\n",
      "Epoch 1/35: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [00:20<00:00,  9.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/35], Train Loss: 0.7915138806, Valid Loss: 0.3948387229\n",
      "Save new model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/35: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [00:14<00:00, 14.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/35], Train Loss: 0.2599650022, Valid Loss: 0.1713597536\n",
      "Save new model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/35: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [00:14<00:00, 13.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/35], Train Loss: 0.1288312185, Valid Loss: 0.1159318684\n",
      "Save new model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/35: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [00:14<00:00, 13.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/35], Train Loss: 0.0932643483, Valid Loss: 0.0867765307\n",
      "Save new model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/35: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [00:15<00:00, 13.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/35], Train Loss: 0.0730143409, Valid Loss: 0.1086332097\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/35: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [00:15<00:00, 13.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/35], Train Loss: 0.0632361531, Valid Loss: 0.0758154777\n",
      "Save new model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/35: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [00:15<00:00, 13.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/35], Train Loss: 0.0520622558, Valid Loss: 0.0625697410\n",
      "Save new model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/35: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [00:14<00:00, 13.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/35], Train Loss: 0.0496957882, Valid Loss: 0.0640980786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/35: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [00:14<00:00, 13.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/35], Train Loss: 0.0476635729, Valid Loss: 0.0621889183\n",
      "Save new model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/35: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [00:15<00:00, 13.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/35], Train Loss: 0.0409848677, Valid Loss: 0.0543381970\n",
      "Save new model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/35: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [00:15<00:00, 13.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/35], Train Loss: 0.0426197103, Valid Loss: 0.0558440489\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/35: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [00:15<00:00, 13.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/35], Train Loss: 0.0374687874, Valid Loss: 0.0729330898\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/35: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [00:15<00:00, 13.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/35], Train Loss: 0.0399689138, Valid Loss: 0.0571078590\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/35: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [00:15<00:00, 13.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/35], Train Loss: 0.0346623320, Valid Loss: 0.0538507759\n",
      "Save new model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/35: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [00:15<00:00, 13.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/35], Train Loss: 0.0289390503, Valid Loss: 0.0576547151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/35: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [00:15<00:00, 13.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/35], Train Loss: 0.0323222343, Valid Loss: 0.0588104189\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/35: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [00:15<00:00, 13.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17/35], Train Loss: 0.0277758757, Valid Loss: 0.0599497568\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/35: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [00:15<00:00, 13.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18/35], Train Loss: 0.0255504111, Valid Loss: 0.0577437467\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/35: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [00:15<00:00, 13.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19/35], Train Loss: 0.0252469746, Valid Loss: 0.0685134076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/35: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [00:15<00:00, 13.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/35], Train Loss: 0.0295202299, Valid Loss: 0.0631089046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/35: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [00:15<00:00, 13.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [21/35], Train Loss: 0.0259632046, Valid Loss: 0.0672260444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/35: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [00:15<00:00, 13.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [22/35], Train Loss: 0.0220012451, Valid Loss: 0.0619535535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/35: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [00:15<00:00, 13.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [23/35], Train Loss: 0.0293731510, Valid Loss: 0.0563209732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/35: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [00:15<00:00, 13.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [24/35], Train Loss: 0.0304717052, Valid Loss: 0.0530913005\n",
      "Save new model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/35: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [00:15<00:00, 13.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [25/35], Train Loss: 0.0212959274, Valid Loss: 0.0596857323\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/35: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [00:15<00:00, 13.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [26/35], Train Loss: 0.0198096261, Valid Loss: 0.0591736212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/35: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [00:15<00:00, 13.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [27/35], Train Loss: 0.0184407332, Valid Loss: 0.0566628293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/35: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [00:15<00:00, 13.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [28/35], Train Loss: 0.0237125706, Valid Loss: 0.0615448957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/35: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [00:15<00:00, 13.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [29/35], Train Loss: 0.0183010276, Valid Loss: 0.0607594093\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30/35: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [00:15<00:00, 13.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [30/35], Train Loss: 0.0194363485, Valid Loss: 0.0597591778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31/35: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [00:15<00:00, 13.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [31/35], Train Loss: 0.0160440364, Valid Loss: 0.0604716638\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32/35: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [00:15<00:00, 13.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [32/35], Train Loss: 0.0162495608, Valid Loss: 0.0631271529\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33/35: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [00:15<00:00, 13.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [33/35], Train Loss: 0.0190533662, Valid Loss: 0.0666143914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34/35: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [00:15<00:00, 13.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [34/35], Train Loss: 0.0196132478, Valid Loss: 0.0641676676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35/35: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [00:15<00:00, 13.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [35/35], Train Loss: 0.0171379529, Valid Loss: 0.0690819461\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "import segmentation_models_pytorch as smp\n",
    "from torchinfo import summary\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "\n",
    "# Constants\n",
    "NUM_CLASSES = 3\n",
    "IMAGE_SIZE = (256, 256)\n",
    "LEARNING_RATE = 0.0001\n",
    "BATCH_SIZE = 4\n",
    "NUM_EPOCHS = 35\n",
    "\n",
    "# Data paths\n",
    "TRAIN_PATH = '/kaggle/input/bkai-igh-neopolyp/train/train'\n",
    "TRAIN_MASK_PATH = '/kaggle/input/bkai-igh-neopolyp/train_gt/train_gt'\n",
    "\n",
    "color_dict = {\n",
    "    0: [0, 0, 0],  # Background\n",
    "    1: [255, 0, 0],  # Class 1 (Red)\n",
    "    2: [0, 255, 0],  # Class 2 (Green)\n",
    "    # Add more classes if necessary\n",
    "}\n",
    "# Utility Functions\n",
    "\n",
    "def mask_to_rgb(mask, color_dict):\n",
    "    \"\"\"Converts mask to RGB image.\"\"\"\n",
    "    output = np.zeros((mask.shape[0], mask.shape[1], 3))\n",
    "\n",
    "    for k in color_dict.keys():\n",
    "        output[mask==k] = color_dict[k]\n",
    "\n",
    "    return np.uint8(output)\n",
    "\n",
    "def save_best_model(epoch, model, optimizer, loss, save_path):\n",
    "    \"\"\"Saves the best model.\"\"\"\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model': model.state_dict(),\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "        'loss': loss,\n",
    "    }\n",
    "    torch.save(checkpoint, save_path)\n",
    "\n",
    "def get_device():\n",
    "    \"\"\"Returns the appropriate device (GPU or CPU).\"\"\"\n",
    "    return torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Dataset Classes\n",
    "\n",
    "class UnetImageDataset(Dataset):\n",
    "    \"\"\"Custom dataset for images and labels.\"\"\"\n",
    "    def __init__(self, img_dir, label_dir, resize=None, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img_dir (str): Directory containing input images.\n",
    "            label_dir (str): Directory containing corresponding label masks.\n",
    "            resize (tuple): Desired image size (height, width).\n",
    "            transform (callable): Optional transform to be applied to the image.\n",
    "        \"\"\"\n",
    "        self.img_dir = img_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.resize = resize\n",
    "        self.transform = transform\n",
    "        self.images = os.listdir(self.img_dir)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the number of images in the dataset.\"\"\"\n",
    "        return len(self.images)\n",
    "\n",
    "    def read_mask(self, mask_path):\n",
    "        \"\"\"Reads and processes the mask image.\"\"\"\n",
    "        image = cv2.imread(mask_path)\n",
    "        image = cv2.resize(image, self.resize)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "        lower1 = np.array([0, 100, 20])\n",
    "        upper1 = np.array([10, 255, 255])\n",
    "\n",
    "        lower2 = np.array([160,100,20])\n",
    "        upper2 = np.array([179,255,255])\n",
    "        lower_mask = cv2.inRange(image, lower1, upper1)\n",
    "        upper_mask = cv2.inRange(image, lower2, upper2)\n",
    "        \n",
    "        red_mask = lower_mask + upper_mask;\n",
    "        red_mask[red_mask != 0] = 1\n",
    "\n",
    "        green_mask = cv2.inRange(image, (36, 25, 25), (70, 255, 255))\n",
    "        green_mask[green_mask != 0] = 2\n",
    "\n",
    "        full_mask = cv2.bitwise_or(red_mask, green_mask)\n",
    "        full_mask = np.expand_dims(full_mask, axis=-1) \n",
    "        full_mask = full_mask.astype(np.uint8)\n",
    "        \n",
    "        return full_mask\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Gets an image and its corresponding label at the given index.\"\"\"\n",
    "        img_path = os.path.join(self.img_dir, self.images[idx])\n",
    "        label_path = os.path.join(self.label_dir, self.images[idx])\n",
    "\n",
    "        # Read and preprocess the image\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image = cv2.resize(image, self.resize)\n",
    "\n",
    "        # Read and preprocess the label\n",
    "        label = self.read_mask(label_path)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "        \n",
    "    def show_image(self, idx):\n",
    "        \"\"\"Displays the original image and its label.\"\"\"\n",
    "        img_path = os.path.join(self.img_dir, self.images[idx])\n",
    "        label_path = os.path.join(self.label_dir, self.images[idx])\n",
    "\n",
    "        image = plt.imread(img_path)\n",
    "        label = plt.imread(label_path)\n",
    "\n",
    "        fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "        axs[0].imshow(image)\n",
    "        axs[0].set_title('Image')\n",
    "        axs[1].imshow(label)\n",
    "        axs[1].set_title('Label')\n",
    "        plt.show()\n",
    "\n",
    "# Extended Dataset Class\n",
    "\n",
    "class UnetDataset(UnetImageDataset):\n",
    "    \"\"\"Extended dataset class.\"\"\"\n",
    "    def __init__(self, data, targets, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data (list): List of input images.\n",
    "            targets (list): List of corresponding label masks.\n",
    "            transform (callable): Optional transform to be applied to the image.\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.targets = targets\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Gets an image and its corresponding label at the given index.\"\"\"\n",
    "        image = self.data[index]\n",
    "        label = self.targets[index]\n",
    "\n",
    "        if self.transform:\n",
    "            transformed = self.transform(image=image, mask=label)\n",
    "            image = transformed['image'].float()\n",
    "            label = transformed['mask'].float()\n",
    "            label = label.permute(2, 0, 1)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the number of images in the dataset.\"\"\"\n",
    "        return len(self.data)\n",
    "\n",
    "# Data Augmentation\n",
    "\n",
    "train_transform = A.Compose([\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.5),\n",
    "    A.RandomGamma(gamma_limit=(70, 130), eps=None, always_apply=False, p=0.2),\n",
    "    A.RGBShift(p=0.3, r_shift_limit=10, g_shift_limit=10, b_shift_limit=10),\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "val_transform = A.Compose([\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "# Dataset Split and Creation\n",
    "\n",
    "image_path = [os.path.join(root, file) for root, _, files in os.walk(TRAIN_PATH) for file in files]\n",
    "mask_path = [os.path.join(root, file) for root, _, files in os.walk(TRAIN_MASK_PATH) for file in files]\n",
    "\n",
    "dataset = UnetImageDataset(\n",
    "    img_dir=TRAIN_PATH,\n",
    "    label_dir=TRAIN_MASK_PATH,\n",
    "    resize=(256, 256),\n",
    "    transform=None\n",
    ")\n",
    "\n",
    "images_data = []\n",
    "labels_data = []\n",
    "for x,y in dataset:\n",
    "    images_data.append(x)\n",
    "    labels_data.append(y)\n",
    "# Model Initialization\n",
    "\n",
    "model = smp.Unet(\n",
    "    encoder_name=\"resnet34\",\n",
    "    encoder_weights=\"imagenet\",\n",
    "    in_channels=3,\n",
    "    classes=NUM_CLASSES\n",
    ")\n",
    "\n",
    "# Data Loader Creation\n",
    "\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset = UnetDataset(images_data[:train_size], labels_data[:train_size], transform=train_transform)\n",
    "val_dataset = UnetDataset(images_data[train_size:], labels_data[train_size:], transform=val_transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# Model Training\n",
    "\n",
    "device = get_device()\n",
    "model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "wandb.login(\n",
    "    # set the wandb project where this run will be logged\n",
    "#     project= \"PolypSegment\", \n",
    "    key = \"b9575849263a9312a73f76d71d270c8751628e10\",\n",
    ")\n",
    "wandb.init(project='Unet_polyp-Segmentation')\n",
    "\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    train_loss_epoch = 0  # Initialize train_loss_epoch\n",
    "    for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS}\"):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        labels = labels.squeeze(dim=1).long()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        train_loss_epoch += loss.item()  # Accumulate training loss for the epoch\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            labels = labels.squeeze(dim=1).long()\n",
    "            outputs = model(images)\n",
    "            val_loss += criterion(outputs.float(), labels.long()).item()\n",
    "\n",
    "    avg_train_loss = train_loss_epoch / len(train_loader)  # Calculate average training loss\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    print(f\"Epoch [{epoch+1}/{NUM_EPOCHS}], Train Loss: {avg_train_loss:.10f}, Valid Loss: {avg_val_loss:.10f}\")\n",
    "\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        save_best_model(epoch, model, optimizer, val_loss, 'colorization_model.pth')\n",
    "        print('Save new model')\n",
    "    wandb.log({\"Train loss\": avg_train_loss, \"Valid loss\": avg_val_loss})\n",
    "    \n",
    "# Model Inference on Test Data\n",
    "\n",
    "trainsize = 256\n",
    "model.eval()\n",
    "\n",
    "for i in os.listdir(\"/kaggle/input/bkai-igh-neopolyp/test/test\"):\n",
    "    img_path = os.path.join(\"/kaggle/input/bkai-igh-neopolyp/test/test\", i)\n",
    "    ori_img = cv2.imread(img_path)\n",
    "    ori_img = cv2.cvtColor(ori_img, cv2.COLOR_BGR2RGB)\n",
    "    ori_w = ori_img.shape[0]\n",
    "    ori_h = ori_img.shape[1]\n",
    "    img = cv2.resize(ori_img, (trainsize, trainsize))\n",
    "    transformed = val_transform(image=img)\n",
    "    input_img = transformed[\"image\"]\n",
    "    input_img = input_img.unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        output_mask = model.forward(input_img).squeeze(0).cpu().numpy().transpose(1, 2, 0)\n",
    "    mask = cv2.resize(output_mask, (ori_h, ori_w))\n",
    "    mask = np.argmax(mask, axis=2)\n",
    "    new_rgb_mask = np.zeros((*mask.shape, 3)).astype(np.uint8)\n",
    "    mask_rgb = mask_to_rgb(mask, color_dict)\n",
    "    cv2.imwrite(\"predicted_mask/{}\".format(i), mask_rgb)\n",
    "    print(+1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "764b1787",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-15T17:49:00.040089Z",
     "iopub.status.busy": "2023-11-15T17:49:00.039406Z",
     "iopub.status.idle": "2023-11-15T17:49:00.046487Z",
     "shell.execute_reply": "2023-11-15T17:49:00.045380Z"
    },
    "papermill": {
     "duration": 0.307169,
     "end_time": "2023-11-15T17:49:00.048417",
     "exception": false,
     "start_time": "2023-11-15T17:48:59.741248",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "transform = A.Compose([\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.5),\n",
    "    A.RandomGamma(gamma_limit=(70, 130), eps=None, always_apply=False, p=0.2),\n",
    "    A.RGBShift(p=0.3, r_shift_limit=10, g_shift_limit=10, b_shift_limit=10),\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    ToTensorV2(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a88edf5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-15T17:49:00.651272Z",
     "iopub.status.busy": "2023-11-15T17:49:00.650921Z",
     "iopub.status.idle": "2023-11-15T17:49:00.659409Z",
     "shell.execute_reply": "2023-11-15T17:49:00.658511Z"
    },
    "papermill": {
     "duration": 0.310792,
     "end_time": "2023-11-15T17:49:00.661408",
     "exception": false,
     "start_time": "2023-11-15T17:49:00.350616",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "class UNetTestDataClass(Dataset):\n",
    "    def __init__(self, img_dir, transform=None, target_size=(256, 256)):\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.target_size = target_size\n",
    "        self.images = os.listdir(self.img_dir)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_path = os.path.join(self.img_dir, self.images[index])\n",
    "        pil_image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        # Resize image\n",
    "        pil_image = pil_image.resize(self.target_size, Image.BILINEAR)\n",
    "\n",
    "        h, w = self.target_size\n",
    "\n",
    "        # Convert PIL Image to numpy array\n",
    "        img_array = np.array(pil_image)\n",
    "\n",
    "        # Apply transformations\n",
    "        transformed_data = self.transform(image=img_array)\n",
    "        data = transformed_data[\"image\"] / 255  # Divide by 255 after applying the transformation\n",
    "\n",
    "        return data, img_path, h, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1542c13",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-15T17:49:01.256738Z",
     "iopub.status.busy": "2023-11-15T17:49:01.256357Z",
     "iopub.status.idle": "2023-11-15T17:49:01.261864Z",
     "shell.execute_reply": "2023-11-15T17:49:01.261120Z"
    },
    "papermill": {
     "duration": 0.304724,
     "end_time": "2023-11-15T17:49:01.263754",
     "exception": false,
     "start_time": "2023-11-15T17:49:00.959030",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "path = '/kaggle/input/bkai-igh-neopolyp/test/test/'\n",
    "unet_test_dataset = UNetTestDataClass(path, transform)\n",
    "test_dataloader = DataLoader(unet_test_dataset, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bfa93bc5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-15T17:49:01.910692Z",
     "iopub.status.busy": "2023-11-15T17:49:01.910015Z",
     "iopub.status.idle": "2023-11-15T17:49:02.059891Z",
     "shell.execute_reply": "2023-11-15T17:49:02.059041Z"
    },
    "papermill": {
     "duration": 0.453199,
     "end_time": "2023-11-15T17:49:02.062302",
     "exception": false,
     "start_time": "2023-11-15T17:49:01.609103",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i, (data, path, h, w) in enumerate(test_dataloader):\n",
    "    img = data\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "52db4640",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-15T17:49:02.682976Z",
     "iopub.status.busy": "2023-11-15T17:49:02.682548Z",
     "iopub.status.idle": "2023-11-15T17:49:07.600656Z",
     "shell.execute_reply": "2023-11-15T17:49:07.599831Z"
    },
    "papermill": {
     "duration": 5.226689,
     "end_time": "2023-11-15T17:49:07.602891",
     "exception": false,
     "start_time": "2023-11-15T17:49:02.376202",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.transforms import Resize, ToPILImage, InterpolationMode\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Assuming 'b' is your input tensor and 'model' is your segmentation model\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Move the model to the specified device\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "if not os.path.isdir(\"/kaggle/working/predicted_masks\"):\n",
    "    os.mkdir(\"/kaggle/working/predicted_masks\")\n",
    "for _, (img, path, H, W) in enumerate(test_dataloader):\n",
    "    a = path\n",
    "    b = img\n",
    "    h = H\n",
    "    w = W\n",
    "    \n",
    "    # Move input tensors to the device\n",
    "    b = b.to(device)\n",
    "    h = h.to(device)\n",
    "    w = w.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        predicted_mask = model(b)\n",
    "    for i in range(len(a)):\n",
    "        image_id = a[i].split('/')[-1].split('.')[0]\n",
    "        filename = image_id + \".png\"\n",
    "        mask2img = Resize((h[i].item(), w[i].item()), interpolation=InterpolationMode.NEAREST)(ToPILImage()(F.one_hot(torch.argmax(predicted_mask[i], 0)).permute(2, 0, 1).float()))\n",
    "        mask2img.save(os.path.join(\"/kaggle/working/predicted_masks/\", filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b9d28ed7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-15T17:49:08.204371Z",
     "iopub.status.busy": "2023-11-15T17:49:08.204022Z",
     "iopub.status.idle": "2023-11-15T17:49:08.430145Z",
     "shell.execute_reply": "2023-11-15T17:49:08.429351Z"
    },
    "papermill": {
     "duration": 0.529268,
     "end_time": "2023-11-15T17:49:08.432303",
     "exception": false,
     "start_time": "2023-11-15T17:49:07.903035",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working/predicted_masks/4417fda8019410b1fcf0625f608b4ce9.png\n",
      "/kaggle/working/predicted_masks/c695325ded465efde988dfb96d081533.png\n",
      "/kaggle/working/predicted_masks/82ea2c193ac8d551c149b60f2965341c.png\n",
      "/kaggle/working/predicted_masks/1c0e9082ea2c193ac8d551c149b60f29.png\n",
      "/kaggle/working/predicted_masks/e1797c77826f9a7021bab9fc73303988.png\n",
      "/kaggle/working/predicted_masks/7af2ed9fbb63b28163a745959c039830.png\n",
      "/kaggle/working/predicted_masks/aafac813fe3ccba3e032dd2948a80c64.png\n",
      "/kaggle/working/predicted_masks/eecd70ebce6347c491b37c8c2e5a64a8.png\n",
      "/kaggle/working/predicted_masks/7f0019f7e6af7d7147763bdfb928d788.png\n",
      "/kaggle/working/predicted_masks/0a0317371a966bf4b3466463a3c64db1.png\n",
      "/kaggle/working/predicted_masks/dc0bb223c4eaf3372eae567c94ea04c6.png\n",
      "/kaggle/working/predicted_masks/45b21960c94b0aab4c024a573c692195.png\n",
      "/kaggle/working/predicted_masks/f8e26031fbb5e52c41545ba55aadaa77.png\n",
      "/kaggle/working/predicted_masks/d6bf62f215f0da4ad3a7ab8df9da7386.png\n",
      "/kaggle/working/predicted_masks/3c84417fda8019410b1fcf0625f608b4.png\n",
      "/kaggle/working/predicted_masks/13dd311a65d2b46d0a6085835c525af6.png\n",
      "/kaggle/working/predicted_masks/80cae6daedd989517cb8041ed86e5822.png\n",
      "/kaggle/working/predicted_masks/c656702fa602bb3c7abacdbd7e6afd56.png\n",
      "/kaggle/working/predicted_masks/780fd497e1c0e9082ea2c193ac8d551c.png\n",
      "/kaggle/working/predicted_masks/cb2eb1ef57af2ed9fbb63b28163a7459.png\n",
      "/kaggle/working/predicted_masks/e7998934d417cb2eb1ef57af2ed9fbb6.png\n",
      "/kaggle/working/predicted_masks/26679bff55177a34fc01019eec999fd8.png\n",
      "/kaggle/working/predicted_masks/c7e610b1531871f2fd85a04faeeb2b53.png\n",
      "/kaggle/working/predicted_masks/4ca6160127cd1d5ff99c267599fc487b.png\n",
      "/kaggle/working/predicted_masks/a9d45c3dbc695325ded465efde988dfb.png\n",
      "/kaggle/working/predicted_masks/5c1346e62522325c1b9c4fc9cbe1eca1.png\n",
      "/kaggle/working/predicted_masks/782707d7c359e27888daefee82519763.png\n",
      "/kaggle/working/predicted_masks/dc70626ab4ec3d46e602b296cc5cfd26.png\n",
      "/kaggle/working/predicted_masks/3b8318ecf467d7ad048df39beb176363.png\n",
      "/kaggle/working/predicted_masks/710d568df17586ad8f3297c819c90895.png\n",
      "/kaggle/working/predicted_masks/cc5cfd263f1f90be28799235026b3550.png\n",
      "/kaggle/working/predicted_masks/6ad1468996b4a9ce6d840b53a6558038.png\n",
      "/kaggle/working/predicted_masks/dd094a7f32574d6c748c41743c6c08a1.png\n",
      "/kaggle/working/predicted_masks/60a633a8d5b2b2b55157b7781e2c706c.png\n",
      "/kaggle/working/predicted_masks/8b8ec74baddc22268d4b4ef4d95ceea1.png\n",
      "/kaggle/working/predicted_masks/5beb48f0be11d0309d1dff09b8405734.png\n",
      "/kaggle/working/predicted_masks/afe1f119f21b248d152b672ab3492fc6.png\n",
      "/kaggle/working/predicted_masks/87133b51209db6dcdda5cc8a788edaeb.png\n",
      "/kaggle/working/predicted_masks/343f27ebc5d92b9076135d76d0bbd4ce.png\n",
      "/kaggle/working/predicted_masks/3c3ca4d5060a633a8d5b2b2b55157b77.png\n",
      "/kaggle/working/predicted_masks/625559c7e610b1531871f2fd85a04fae.png\n",
      "/kaggle/working/predicted_masks/50534bca540e24f489284b8e6953ad88.png\n",
      "/kaggle/working/predicted_masks/63b8318ecf467d7ad048df39beb17636.png\n",
      "/kaggle/working/predicted_masks/98da48d679d7c7c8d3d96fb2b87fbbcf.png\n",
      "/kaggle/working/predicted_masks/6ddca6ee1af35b65bd9ea42cfcfedb5e.png\n",
      "/kaggle/working/predicted_masks/df8e26031fbb5e52c41545ba55aadaa7.png\n",
      "/kaggle/working/predicted_masks/df366e057db382b8564872a27301a654.png\n",
      "/kaggle/working/predicted_masks/692195f853af7f8a4df1ec859759b7c8.png\n",
      "/kaggle/working/predicted_masks/dd78294679c9cbb2a365b5574868eb60.png\n",
      "/kaggle/working/predicted_masks/c193ac8d551c149b60f2965341caf528.png\n",
      "/kaggle/working/predicted_masks/e3c84417fda8019410b1fcf0625f608b.png\n",
      "/kaggle/working/predicted_masks/5a51625559c7e610b1531871f2fd85a0.png\n",
      "/kaggle/working/predicted_masks/7cdf3f33c3ca4d5060a633a8d5b2b2b5.png\n",
      "/kaggle/working/predicted_masks/8fa8625605da2023387fd56c04414eaa.png\n",
      "/kaggle/working/predicted_masks/88e16d4ca6160127cd1d5ff99c267599.png\n",
      "/kaggle/working/predicted_masks/6240619ebebe9e9c9d00a4262b4fe4a5.png\n",
      "/kaggle/working/predicted_masks/72d9e593b6be1ac29adbe86f03d900fd.png\n",
      "/kaggle/working/predicted_masks/ff55177a34fc01019eec999fd84e679b.png\n",
      "/kaggle/working/predicted_masks/461c2a337948a41964c1d4f50a5f3601.png\n",
      "/kaggle/working/predicted_masks/39d6aad6bb0170a40ed32deef71fbe08.png\n",
      "/kaggle/working/predicted_masks/d5060a633a8d5b2b2b55157b7781e2c7.png\n",
      "/kaggle/working/predicted_masks/0a5f3601ad4f13ccf1f4b331a412fc44.png\n",
      "/kaggle/working/predicted_masks/fb905b78a91391adc0bb223c4eaf3372.png\n",
      "/kaggle/working/predicted_masks/af35b65bd9ea42cfcfedb5eb2a0e4b50.png\n",
      "/kaggle/working/predicted_masks/7330398846f67b5df7cdf3f33c3ca4d5.png\n",
      "/kaggle/working/predicted_masks/e4a17af18f72c8e6166a915669c99390.png\n",
      "/kaggle/working/predicted_masks/e8bfb905b78a91391adc0bb223c4eaf3.png\n",
      "/kaggle/working/predicted_masks/e73749a0d21db70dd094a7f32574d6c7.png\n",
      "/kaggle/working/predicted_masks/ca4d5060a633a8d5b2b2b55157b7781e.png\n",
      "/kaggle/working/predicted_masks/8eb5a9a8a8d7fcc9df8e5ad89d284483.png\n",
      "/kaggle/working/predicted_masks/e5e8f14e1e0ae936de314f2d95e6c487.png\n",
      "/kaggle/working/predicted_masks/1db239dda50f954ba59c7de13a35276a.png\n",
      "/kaggle/working/predicted_masks/bec33b5e3d68f9d4c331587f9b9d49e2.png\n",
      "/kaggle/working/predicted_masks/05b78a91391adc0bb223c4eaf3372eae.png\n",
      "/kaggle/working/predicted_masks/d6240619ebebe9e9c9d00a4262b4fe4a.png\n",
      "/kaggle/working/predicted_masks/c41545ba55aadaa77712a48e11d579d9.png\n",
      "/kaggle/working/predicted_masks/7cb2eb1ef57af2ed9fbb63b28163a745.png\n",
      "/kaggle/working/predicted_masks/425b976973f13dd311a65d2b46d0a608.png\n",
      "/kaggle/working/predicted_masks/5b21960c94b0aab4c024a573c692195f.png\n",
      "/kaggle/working/predicted_masks/e9082ea2c193ac8d551c149b60f29653.png\n",
      "/kaggle/working/predicted_masks/f13dd311a65d2b46d0a6085835c525af.png\n",
      "/kaggle/working/predicted_masks/6f67b5df7cdf3f33c3ca4d5060a633a8.png\n",
      "/kaggle/working/predicted_masks/b70dd094a7f32574d6c748c41743c6c0.png\n",
      "/kaggle/working/predicted_masks/4ef4d95ceea11957998906d3694abb47.png\n",
      "/kaggle/working/predicted_masks/f62f215f0da4ad3a7ab8df9da7386835.png\n",
      "/kaggle/working/predicted_masks/626650908b1cb932a767bf5487ced51b.png\n",
      "/kaggle/working/predicted_masks/559c7e610b1531871f2fd85a04faeeb2.png\n",
      "/kaggle/working/predicted_masks/6f4d4987ea3b4bae5672a230194c5a08.png\n",
      "/kaggle/working/predicted_masks/1002ec4a1fe748f3085f1ce88cbdf366.png\n",
      "/kaggle/working/predicted_masks/9632a3c6f7f7fb2a643f15bd0249ddcc.png\n",
      "/kaggle/working/predicted_masks/3dd311a65d2b46d0a6085835c525af63.png\n",
      "/kaggle/working/predicted_masks/6231002ec4a1fe748f3085f1ce88cbdf.png\n",
      "/kaggle/working/predicted_masks/7fda8019410b1fcf0625f608b4ce9762.png\n",
      "/kaggle/working/predicted_masks/a6d9ba9d45c3dbc695325ded465efde9.png\n",
      "/kaggle/working/predicted_masks/d077bad31c8c5f54ffaa27a623511c38.png\n",
      "/kaggle/working/predicted_masks/71f2fd85a04faeeb2b535797395305af.png\n",
      "/kaggle/working/predicted_masks/4fda8daadc8dd23ae214d84b5dec33fd.png\n",
      "/kaggle/working/predicted_masks/391adc0bb223c4eaf3372eae567c94ea.png\n",
      "/kaggle/working/predicted_masks/1209db6dcdda5cc8a788edaeb6aa460a.png\n",
      "/kaggle/working/predicted_masks/e19769fa2d37d32780fd497e1c0e9082.png\n",
      "/kaggle/working/predicted_masks/1b62f15ec83b97bb11e8e0c4416c1931.png\n",
      "/kaggle/working/predicted_masks/395e56a6d9ba9d45c3dbc695325ded46.png\n",
      "/kaggle/working/predicted_masks/f14e1e0ae936de314f2d95e6c487ffa6.png\n",
      "/kaggle/working/predicted_masks/eff05dec1eb3a70b145a7d8d3b6c0ed7.png\n",
      "/kaggle/working/predicted_masks/68d4b4ef4d95ceea11957998906d3694.png\n",
      "/kaggle/working/predicted_masks/0626ab4ec3d46e602b296cc5cfd263f1.png\n",
      "/kaggle/working/predicted_masks/94a7f32574d6c748c41743c6c08a1d1a.png\n",
      "/kaggle/working/predicted_masks/cbb2a365b5574868eb60861ee1ff0b8a.png\n",
      "/kaggle/working/predicted_masks/9c7976c1182df0de51d32128c358d1fd.png\n",
      "/kaggle/working/predicted_masks/3657e4314fe384eb2ba3adfda6c1899f.png\n",
      "/kaggle/working/predicted_masks/0fca6a4248a41e8db8b4ed633b456aaa.png\n",
      "/kaggle/working/predicted_masks/4baddc22268d4b4ef4d95ceea1195799.png\n",
      "/kaggle/working/predicted_masks/2a365b5574868eb60861ee1ff0b8a4f6.png\n",
      "/kaggle/working/predicted_masks/6679bff55177a34fc01019eec999fd84.png\n",
      "/kaggle/working/predicted_masks/1531871f2fd85a04faeeb2b535797395.png\n",
      "/kaggle/working/predicted_masks/fdbc3bbc04a3afe1f119f21b248d152b.png\n",
      "/kaggle/working/predicted_masks/7ad1cf2eb9d32a3dc907950289e976c7.png\n",
      "/kaggle/working/predicted_masks/2d9e593b6be1ac29adbe86f03d900fd1.png\n",
      "/kaggle/working/predicted_masks/be4d18d5401f659532897255ce2dd4ae.png\n",
      "/kaggle/working/predicted_masks/e2cd066b9fdbc3bbc04a3afe1f119f21.png\n",
      "/kaggle/working/predicted_masks/7936140a2d5fc1443c4e445927738677.png\n",
      "/kaggle/working/predicted_masks/aeeb2b535797395305af926a6f23c5d6.png\n",
      "/kaggle/working/predicted_masks/8954bb13d3727c7e5e1069646f2f0bb8.png\n",
      "/kaggle/working/predicted_masks/4f437f0019f7e6af7d7147763bdfb928.png\n",
      "/kaggle/working/predicted_masks/c4be73749a0d21db70dd094a7f32574d.png\n",
      "/kaggle/working/predicted_masks/4e2a6e51d077bad31c8c5f54ffaa27a6.png\n",
      "/kaggle/working/predicted_masks/1ad4f13ccf1f4b331a412fc44655fb51.png\n",
      "/kaggle/working/predicted_masks/a15fc656702fa602bb3c7abacdbd7e6a.png\n",
      "/kaggle/working/predicted_masks/3f33c3ca4d5060a633a8d5b2b2b55157.png\n",
      "/kaggle/working/predicted_masks/c22268d4b4ef4d95ceea11957998906d.png\n",
      "/kaggle/working/predicted_masks/268d4b4ef4d95ceea11957998906d369.png\n",
      "/kaggle/working/predicted_masks/318ecf467d7ad048df39beb176363408.png\n",
      "/kaggle/working/predicted_masks/4c1711b62f15ec83b97bb11e8e0c4416.png\n",
      "/kaggle/working/predicted_masks/5664c1711b62f15ec83b97bb11e8e0c4.png\n",
      "/kaggle/working/predicted_masks/cdf3f33c3ca4d5060a633a8d5b2b2b55.png\n",
      "/kaggle/working/predicted_masks/d694539ef2424a9218697283baa3657e.png\n",
      "/kaggle/working/predicted_masks/66e057db382b8564872a27301a654864.png\n",
      "/kaggle/working/predicted_masks/77e004e8bfb905b78a91391adc0bb223.png\n",
      "/kaggle/working/predicted_masks/05734fbeedd0f9da760db74a29abdb04.png\n",
      "/kaggle/working/predicted_masks/936de314f2d95e6c487ffa651b477422.png\n",
      "/kaggle/working/predicted_masks/9fc7330398846f67b5df7cdf3f33c3ca.png\n",
      "/kaggle/working/predicted_masks/54ba59c7de13a35276a476420655433a.png\n",
      "/kaggle/working/predicted_masks/7b5df7cdf3f33c3ca4d5060a633a8d5b.png\n",
      "/kaggle/working/predicted_masks/b21960c94b0aab4c024a573c692195f8.png\n",
      "/kaggle/working/predicted_masks/2ed9fbb63b28163a745959c03983064a.png\n",
      "/kaggle/working/predicted_masks/314fe384eb2ba3adfda6c1899fdc9837.png\n",
      "/kaggle/working/predicted_masks/3bbc04a3afe1f119f21b248d152b672a.png\n",
      "/kaggle/working/predicted_masks/60b246359c68c836f843dcf41f4dce3c.png\n",
      "/kaggle/working/predicted_masks/5e8f14e1e0ae936de314f2d95e6c487f.png\n",
      "/kaggle/working/predicted_masks/8cbdf366e057db382b8564872a27301a.png\n",
      "/kaggle/working/predicted_masks/cf464aa36bf7c09a3bb0e5ca159410b9.png\n",
      "/kaggle/working/predicted_masks/faef7fdb2d45b21960c94b0aab4c024a.png\n",
      "/kaggle/working/predicted_masks/a51625559c7e610b1531871f2fd85a04.png\n",
      "/kaggle/working/predicted_masks/67d4dcf9596154efb7cef748d9cbd617.png\n",
      "/kaggle/working/predicted_masks/fe1f119f21b248d152b672ab3492fc62.png\n",
      "/kaggle/working/predicted_masks/0af3feff05dec1eb3a70b145a7d8d3b6.png\n",
      "/kaggle/working/predicted_masks/15fc656702fa602bb3c7abacdbd7e6af.png\n",
      "/kaggle/working/predicted_masks/85a04faeeb2b535797395305af926a6f.png\n",
      "/kaggle/working/predicted_masks/2cd066b9fdbc3bbc04a3afe1f119f21b.png\n",
      "/kaggle/working/predicted_masks/39dda50f954ba59c7de13a35276a4764.png\n",
      "/kaggle/working/predicted_masks/6d3694abb47953b0e4909384b57bb6a0.png\n",
      "/kaggle/working/predicted_masks/97e1c0e9082ea2c193ac8d551c149b60.png\n",
      "/kaggle/working/predicted_masks/41ed86e58224cb76a67d4dcf9596154e.png\n",
      "/kaggle/working/predicted_masks/ea42b4eebc9e5a87e443434ac60af150.png\n",
      "/kaggle/working/predicted_masks/285e26c90e1797c77826f9a7021bab9f.png\n",
      "/kaggle/working/predicted_masks/a6e51d077bad31c8c5f54ffaa27a6235.png\n",
      "/kaggle/working/predicted_masks/019410b1fcf0625f608b4ce97629ab55.png\n",
      "/kaggle/working/predicted_masks/db5eb2a0e4b50889d874c68c030b9afe.png\n",
      "/kaggle/working/predicted_masks/e56a6d9ba9d45c3dbc695325ded465ef.png\n",
      "/kaggle/working/predicted_masks/a48847ae8395e56a6d9ba9d45c3dbc69.png\n",
      "/kaggle/working/predicted_masks/80c643782707d7c359e27888daefee82.png\n",
      "/kaggle/working/predicted_masks/30c2f4fc276ed9f178dc2f4af6266509.png\n",
      "/kaggle/working/predicted_masks/eb1ef57af2ed9fbb63b28163a745959c.png\n",
      "/kaggle/working/predicted_masks/633a8d5b2b2b55157b7781e2c706c75c.png\n",
      "/kaggle/working/predicted_masks/3425b976973f13dd311a65d2b46d0a60.png\n",
      "/kaggle/working/predicted_masks/0619ebebe9e9c9d00a4262b4fe4a5a95.png\n",
      "/kaggle/working/predicted_masks/5026b3550534bca540e24f489284b8e6.png\n",
      "/kaggle/working/predicted_masks/a3657e4314fe384eb2ba3adfda6c1899.png\n",
      "/kaggle/working/predicted_masks/ad43fe2cd066b9fdbc3bbc04a3afe1f1.png\n",
      "/kaggle/working/predicted_masks/0398846f67b5df7cdf3f33c3ca4d5060.png\n",
      "/kaggle/working/predicted_masks/4e8bfb905b78a91391adc0bb223c4eaf.png\n",
      "/kaggle/working/predicted_masks/27738677a6b1f2c6d40b3bbba8f6c704.png\n",
      "/kaggle/working/predicted_masks/fcd6da15fc656702fa602bb3c7abacdb.png\n",
      "/kaggle/working/predicted_masks/998906d3694abb47953b0e4909384b57.png\n",
      "/kaggle/working/predicted_masks/677a6b1f2c6d40b3bbba8f6c704801b3.png\n",
      "/kaggle/working/predicted_masks/7f32574d6c748c41743c6c08a1d1ad8f.png\n",
      "/kaggle/working/predicted_masks/8395e56a6d9ba9d45c3dbc695325ded4.png\n",
      "/kaggle/working/predicted_masks/d3694abb47953b0e4909384b57bb6a05.png\n",
      "/kaggle/working/predicted_masks/be86f03d900fd197cd955fa095f97845.png\n",
      "/kaggle/working/predicted_masks/02fa602bb3c7abacdbd7e6afd56ea7bc.png\n",
      "/kaggle/working/predicted_masks/c5a0808bee60b246359c68c836f843dc.png\n",
      "/kaggle/working/predicted_masks/f7fdb2d45b21960c94b0aab4c024a573.png\n",
      "/kaggle/working/predicted_masks/6b83ef461c2a337948a41964c1d4f50a.png\n",
      "/kaggle/working/predicted_masks/a6a4248a41e8db8b4ed633b456aaafac.png\n",
      "/kaggle/working/predicted_masks/cb1b387133b51209db6dcdda5cc8a788.png\n",
      "/kaggle/working/predicted_masks/e1e0ae936de314f2d95e6c487ffa651b.png\n",
      "/kaggle/working/predicted_masks/f8e5ad89d2844837f2a0f1536ad3f6a5.png\n",
      "/kaggle/working/predicted_masks/cf6644589e532a9ee954f81faedbce39.png\n",
      "/kaggle/working/predicted_masks/ff05dec1eb3a70b145a7d8d3b6c0ed75.png\n",
      "/kaggle/working/predicted_masks/3c692195f853af7f8a4df1ec859759b7.png\n"
     ]
    }
   ],
   "source": [
    "def rle_to_string(runs):\n",
    "    return ' '.join(str(x) for x in runs)\n",
    "\n",
    "def rle_encode_one_mask(mask):\n",
    "    pixels = mask.flatten()\n",
    "    pixels[pixels > 0] = 255\n",
    "    use_padding = False\n",
    "    if pixels[0] or pixels[-1]:\n",
    "        use_padding = True\n",
    "        pixel_padded = np.zeros([len(pixels) + 2], dtype=pixels.dtype)\n",
    "        pixel_padded[1:-1] = pixels\n",
    "        pixels = pixel_padded\n",
    "    \n",
    "    rle = np.where(pixels[1:] != pixels[:-1])[0] + 2\n",
    "    if use_padding:\n",
    "        rle = rle - 1\n",
    "    rle[1::2] = rle[1::2] - rle[:-1:2]\n",
    "    return rle_to_string(rle)\n",
    "\n",
    "def mask2string(dir):\n",
    "    ## mask --> string\n",
    "    strings = []\n",
    "    ids = []\n",
    "    ws, hs = [[] for i in range(2)]\n",
    "    for image_id in os.listdir(dir):\n",
    "        id = image_id.split('.')[0]\n",
    "        path = os.path.join(dir, image_id)\n",
    "        print(path)\n",
    "        img = cv2.imread(path)[:,:,::-1]\n",
    "        h, w = img.shape[0], img.shape[1]\n",
    "        for channel in range(2):\n",
    "            ws.append(w)\n",
    "            hs.append(h)\n",
    "            ids.append(f'{id}_{channel}')\n",
    "            string = rle_encode_one_mask(img[:,:,channel])\n",
    "            strings.append(string)\n",
    "    r = {\n",
    "        'ids': ids,\n",
    "        'strings': strings,\n",
    "    }\n",
    "    return r\n",
    "\n",
    "\n",
    "MASK_DIR_PATH = '/kaggle/working/predicted_masks' # change this to the path to your output mask folder\n",
    "dir = MASK_DIR_PATH\n",
    "res = mask2string(dir)\n",
    "df = pd.DataFrame(columns=['Id', 'Expected'])\n",
    "df['Id'] = res['ids']\n",
    "df['Expected'] = res['strings']\n",
    "df.to_csv(r'output.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b88c96",
   "metadata": {
    "papermill": {
     "duration": 0.319386,
     "end_time": "2023-11-15T17:49:09.063836",
     "exception": false,
     "start_time": "2023-11-15T17:49:08.744450",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 2715462,
     "sourceId": 30892,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30580,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 720.265995,
   "end_time": "2023-11-15T17:49:12.201491",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-11-15T17:37:11.935496",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
